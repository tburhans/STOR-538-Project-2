---
title: "cross-validation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(caret)
library(tidyverse)
library(randomForest)
library(neuralnet)
library(Metrics)
library(MASS)
library(glmnet)
library(rpart)
```

```{r}
#reading in data
data <- read_csv(file= 'FinalDataset.csv') %>% subset(GAME_YEAR>=2014)
#data <- data[, -c(1,2,3,4,5,10,11, 13, 14, 15, 16, 17, 18)]
data <- data[, -c(1,2,3,4,5,6,7)]
head(data)

```
Model Building here
```{r}

#split into training and testing based on 80-20 split
set.seed(5)
trn_index = sample(nrow(data), size=0.8*nrow(data))
trn = data[trn_index, ]
tst = data[-trn_index, ]
```

```{r}
#Lasso Regression
#for predicting Total

```

```{r}
#random forest
#for OREB

```


```{r}

```


```{r}

backwards_poisson_model_oreb = glm(OREB ~ PTS_away + TO_home + TO_away + FGA_home + FGA_away + FTA_home + FTA_away + Home_Possesions + Away_Possesions + Home_Points_Per_Possesion + Home_OREB_Percentage + Away_OREB_Percentage + Away_DREB_Percentage, data=trn_model, family= "poisson")

backwards_quasipoisson_model_oreb = glm(OREB ~ PTS_away + TO_home + TO_away + FGA_home + FGA_away + FTA_home + FTA_away + Home_Possesions + Away_Possesions + Home_Points_Per_Possesion + Home_OREB_Percentage + Away_OREB_Percentage + Away_DREB_Percentage, data=trn_model, family= "quasipoisson")

neg_bin_model_oreb = glm.nb(OREB~., data=trn)

rf_model_oreb = randomForest(OREB~., data=trn)


model_backwards_poisson_total = glm(Total ~ OREB_home + OREB_away + TO_home + TO_away + FGA_home + FGA_away + FTA_home + FTA_away + Home_Points_Per_Possesion + Away_Points_Per_Possesion, data=trn_model, family="poisson")




```



This code splits our entire dataset into training and testing data randomly with an 80-20 split of training to testing. Then, it performs k-fold cross-validation (right now k=5). This means that it takes our training dataset, splits it into k chunks, and performs k folds or iterations. Within each fold, it will designate k-1 of these chunks as estimation data, to which it fits a model. One of the chunks will be the validation data. Then, it determines the RMSE of the model's predictions on the validation data. This is done k times for each fold, where each fold has a different validation dataset and different evaluation dataset. Each chunk gets used for validation exactly once. 

Used the caret libraries createFolds function to determine the indexes of the validation data (called the fold_idx) for each fold. This means that fold 1's validation data is different from fold 2's validation data, which is different from fold 3's validation data, etc. Then, the calculate_rmse_single_fold function takes each unique fold validation dataset and estimation dataset, fits the model on the estimation set, predicts on the validation set, and calculates the RMSE of the predictions. Then, the RMSE's of the k-folds are averaged. 


RMSE's of the models using k-fold (have to set Seed if want to reproduce results):

SPREAD:
LM w Backwards Elim (Fab) = 0.8537747
LASSO (Raj) = 0.8989422
RF (Rajee) = 3.454581

TOTAL:
LASSO (Raj) = 1.105374
Poisson (Lex) = 1.216675
Poisson (Tucker) = 1.241033 
Quasipoisson (Tucker) = 1.241033
Negative Binomial (Tucker) = 1.356194
LASSO w Poisson (Raj) = 1.363492
Random Forest (Tucker) = 3.904634
Neural Net (Lex) = 210.83

OREB:
LASSO (Raj) = 0.1463802
Negative Binomial w Backwards Elim (Raj) = 0.8846661
LASSO w Poisson = 0.9980644
Poisson w Backwards Elim (Raj) = 1.008639
Quasipoisson w Backwards Elim (Raj) = 1.008639
Random Forest (Raj) = 1.716421


```{r}
###K-Fold for Models except Lasso

#caret create folds
fold_idx = createFolds(trn$Total, k = 5)

calculate_rmse_single_fold = function(val_idx) {
  #splitting into estimation and validation within current fold
  est=trn[-val_idx, ]
  val=trn[val_idx, ]
  
  #fitting model with est
  mod = glm(OREB ~ PTS_away + TO_home + TO_away + FGA_home + FGA_away + FTA_home + FTA_away + Home_Possesions + Away_Possesions + Home_Points_Per_Possesion + Home_OREB_Percentage + Away_OREB_Percentage + Away_DREB_Percentage, data=est, family= "poisson")
  
  #making predictions with val
  pred = predict(mod, val, type="response")
  
  #calculating RMSE
  #sqrt(mean((pred-val$Total) ^ 2))
  rmse(val$Total, pred)
  
}

results = sapply(fold_idx, calculate_rmse_single_fold)
results
mean(results)

#checking on testing dataset
mod2 = glm(OREB ~ PTS_away + TO_home + TO_away + FGA_home + FGA_away + FTA_home + FTA_away + Home_Possesions + Away_Possesions + Home_Points_Per_Possesion + Home_OREB_Percentage + Away_OREB_Percentage + Away_DREB_Percentage, data=trn, family= "poisson")
pred2 = predict(mod2, tst, type="response")
rmse(tst$Total, pred2)


```

```{r}
###LASSO K FOLD

#caret create folds
fold_idx_lasso = createFolds(trn$Spread, k =5)

lasso_calculate_rmse_single_fold = function(val_idx_lasso) {
  #splitting into estimation and validation within current fold
  est_lasso = trn[-val_idx_lasso, ]
  val_lasso = trn[val_idx_lasso, ]
  
  #lasso split into x and y
  x_trn <- model.matrix(Spread~., est_lasso)[,-1]
  x_tst <- model.matrix(Spread~., val_lasso)[,-1]

  y_trn <- est_lasso$Spread
  y_tst <- val_lasso$Spread
  
  #cv to find min lambda
  set.seed(5)
  cv.lasso <- cv.glmnet(x_trn, y_trn, alpha=1)
  minlambda = cv.lasso$lambda.min
  
  #fitting model
  lasso_model = glmnet(x_trn, y_trn, alpha=1, lambda=minlambda)
  
  #making predictions
  lasso_pred <- predict(lasso_model, newx=x_tst)
  
  #calcualting RMSE
  rmse(y_tst, lasso_pred)
  
  
}

results = sapply(fold_idx_lasso, lasso_calculate_rmse_single_fold)
results
mean(results)

x_trn_whole <- model.matrix(Total~., trn)[,-1]
x_tst_whole <- model.matrix(Total~., tst)[,-1]

y_trn_whole <- trn$Total
y_tst_whole <- tst$Total

set.seed(5)
cv.lasso.whole <- cv.glmnet(x_trn_whole, y_trn_whole, alpha=1)
minlambda.whole = cv.lasso.whole$lambda.min

lasso_model = glmnet(x_trn_whole, y_trn_whole, alpha=1, lambda=minlambda.whole)

coef(lasso_model)



```

```{r}
###get predictions dataset
predictions <- read.csv(url('https://raw.githubusercontent.com/SuperMarioGiacomazzo/STOR538_WEBSITE/master/Playoffs/Round%202/Predictions.csv'))2
```

```{r}
### Function to take in values from predictions dataset, and average every teams statistics
final_data_full <- read.csv(file='FinalDataset.csv')
final_data_full

find_averages = function(home_team, away_team, years, predictor) {
  #subsetting based on year
  dataset = subset(final_data_full, GAME_YEAR >= years)
  
  #subsetting based on home team and away team
  home_games <- subset(dataset, dataset$`Home.Team` == home_team)
  away_games <- subset(dataset, dataset$`Away.Team` == away_team)
  
  #subsetting columns based on predictor to get only the 
  #regressors we need
  if (predictor == 'Spread' | predictor == 'Total') {
    home_games <- home_games[, -c(1,2,3,4,5,6,7)]
    away_games <- away_games[, -c(1,2,3,4,5,6,7)]
  } else {
    home_games <- home_games[, -c(1,2,3,4,5,10,11, 13, 14, 15, 16, 17, 18)]
    away_games <- away_games[, -c(1,2,3,4,5,10,11, 13, 14, 15, 16, 17, 18)]
  }

  #taking averages
  x<- colMeans(home_games[sapply(home_games, is.numeric)], na.rm = TRUE)
  x<-as.data.frame(t(x))
  
  y<- colMeans(away_games[sapply(away_games, is.numeric)], na.rm = TRUE)
  y <- as.data.frame(t(y))
  
  ###Tucker Start here to pick and choose the appropriate stats from home or away
  ### and put into another dataframe potentially
  #x <- x[, c(10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, )]
}

find_averages_only_home = function(home_team, years, predictor) {
  #subsetting based on year
  dataset = subset(final_data_full, GAME_YEAR >= years)
  
  #subsetting based on home team
  home_games <- subset(dataset, dataset$`Home.Team` == home_team)
  
  #subsetting columns based on predictor to get only the 
  #regressors we need
  if (predictor == 'Spread' | predictor == 'Total') {
    home_games <- home_games[, -c(1,2,3,4,5,6,7)]
  } else {
    home_games <- home_games[, -c(1,2,3,4,5,10,11, 13, 14, 15, 16, 17, 18)]
  }
  
  #taking averages
  x<- colMeans(home_games[sapply(home_games, is.numeric)], na.rm = TRUE)
  x<-as.data.frame(t(x))
  return (x)
}

#training our model first
##decision to make: should we train on all data or just past 2 years??
# right now training on all data since 2019

best_model_spread <- lm(Spread~OREB_home + OREB_away + DREB_home + DREB_away + 
    AST_home + AST_away + STL_home + BLK_home + BLK_away + TO_home + 
    TO_away + FGA_home + FGA_away + FTA_home + FTA_away + OREB_Starters_home + 
    DREB_Starters_home + AST_Starters_away + STL_Starters_away + 
    BLK_Starters_home + Home_Points_Per_Possesion + Away_Points_Per_Possesion + 
    Home_Turnover_Percentage + Away_Turnover_Percentage + Home_OREB_Percentage + 
    Away_OREB_Percentage + Away_DREB_Percentage + Home_AST_TO_Ratio + 
    Away_AST_TO_Ratio, data = trn)

#this loops through the predictions dataset, grabs each home and away matchup, sends it through
#our function to find averages, and predicts using our best model on the averages

for (i in 1:nrow(predictions)) {
  #get current row's home and away
  row <- predictions[i, ]
  home <- row[, 2]
  away <- row[, 3]
  
  #send to function to find the averages
  avgs <- find_averages_only_home(home, 2019, 'Spread')
  ##predict using our model 
  results <- predict(best_model_spread, avgs)
  predictions[i, 4] = results
}

predictions

```